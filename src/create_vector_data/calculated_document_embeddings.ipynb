{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This notebook is used as an example of building and interacting with a chromadb embeddings database using a locally stored and downloaded embeddings model.\n",
    "\n",
    "Specifically, we will be using the [instructor-xl embedding model](https://huggingface.co/hkunlp/instructor-xl) which looks create an embedding that allows matching between prompts and relevant documents. This is a relatively flexible embedding model and is more closely aligned to our end-goal of a simple proof-of-concept of a Retrieval-Augmented Generation (RAG) approach to using Large Language Models (LLMs).\n",
    "\n",
    "# Method\n",
    "## Approach for embedding\n",
    "To embed the files, we will be turning them into reasonably sized text chunks and storing them in a static, on-disk chromadb vector database which can be loaded and used for searching.\n",
    "\n",
    "This notebook will show some simple walkthroughs of this and the associated calculated_document_embeddings.py script turns this into more consistent functions for re-use and with better documentation.\n",
    "\n",
    "The use-case we are considering here is largely an asymmetric embedding problem where the query text may be a very different size to the stored text for comparison.\n",
    "\n",
    "For similarity measures in the embedding space, we can consider both the cosine similarity and raw dot-product but noting they are each likely to favour different potential answer lengths.\n",
    "\n",
    "## Text chunking strategy\n",
    "We will aim for context-aware text chunks. This can be file / format specific (i.e Markdown, HTML, code, etc)\n",
    "and will allow for some overlap between these text cunks.\n",
    "\n",
    "## Environment setup\n",
    "\n",
    "Note, I'd reccommend you pick a python kernel built to match the requirements specified in pyproject.toml and to use Poetry to manage the dependecies and virtual environment. I have added the pip magic command to install relevant packages in this notebook but it would be unnecessary if you set up an appropriate virtual environment.\n",
    "\n",
    "## Example data\n",
    "I've downloaded various books that are out of copyright. These are:\n",
    "* Moby Dick converted to five formats:\n",
    "  * txt\n",
    "  * docx\n",
    "  * pdf (but with text encoded)\n",
    "  * markdown\n",
    "  * html\n",
    "* Pride and Prejudice as a PDF\n",
    "* Romeo and Juliet as a PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install chromadb langchain PyYAML sentence-transformers pypdf ipykernel unstructured markdown docx2txt tiktoken InstructorEmbedding accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first set up some constants with paths to our data.\n",
    "\n",
    "Please note, these are hardcoded and would need to be adjusted for where you setup your embedding model and your document data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb  # For creating, managing and interacting with the local vector store\n",
    "import sentence_transformers  # Nominal for default sentence_transformers but instructor model used instead\n",
    "import os  # For file-path operators\n",
    "import uuid  # Used for generating unique IDs for document chunks\n",
    "\n",
    "core_working_directory = r\"C:\\Users\\Alex\\Google Drive\\projects\\llama2_retrieval_augmented_generation\"\n",
    "document_data_dir_path = os.path.join(core_working_directory, \"data\", \"documents\")\n",
    "\n",
    "# Note that the name is in the convention for huggingface.co and this model is apache 2.0 licensed.\n",
    "reference_models_directory = r\"F:\\reference_models\"\n",
    "embedding_model_name_used = \"hkunlp/instructor-xl\"\n",
    "embedding_model_path = os.path.join(reference_models_directory, \"embedding_models\", \"instructor-xl\")\n",
    "\n",
    "# Hugging face local example in collab notebook\n",
    "# https://colab.research.google.com/drive/12v2ZBIucDZ-MBTX4VGEMJR4Fxf-EOYN0#scrollTo=JCb7algHVxeI\n",
    "# https://discuss.huggingface.co/t/using-hugging-face-models-with-private-company-data/56403\n",
    "# Parameters\n",
    "llm_model_name_used = \"tiiuae/falcon-7b-instruct\"\n",
    "llm_model_path = os.path.join(reference_models_directory,\n",
    "                              \"large_language_models\",\n",
    "                              \"llama2\",\n",
    "                              \"falcon-7b-instruct\")\n",
    "\n",
    "# Somewhat arbitrary but shouldn't be too large to avoid hitting issues with context window size on various LLMs\n",
    "# without using context window extension methods, llama2 should have a 2048ish token context window\n",
    "# Therefore want to keep chunks small enough that a few chunks of context can be added to a prompt.\n",
    "n_size_in_doc_chunk = 512\n",
    "n_size_in_chunk_overlap = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, the intent it to load the files, turn them into chunks of reasonably sized text, add some meta-data to each chunk and then save it to the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import UnstructuredHTMLLoader\n",
    "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.document_loaders import Docx2txtLoader\n",
    "# from langchain.document_loaders import JSONLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Note, below is pieced together from langchain docs here:\n",
    "# https://python.langchain.com/docs/modules/data_connection/document_transformers/\n",
    "# from langchain.document_loaders import DocxLoader\n",
    "file_endings_considered = {\"markdown\": \"md\",\n",
    "                           \"html\": \"html\",\n",
    "                           \"text\": \"txt\",\n",
    "                           \"pdf\": \"pdf\",\n",
    "                           \"word_doc\": \"docx\"}\n",
    "\n",
    "file_globbers = {file_type: os.path.join(\"**\", f\"*.{file_ending}\")\n",
    "                 for file_type, file_ending in file_endings_considered.items()}\n",
    "\n",
    "# Build a markdown file loader\n",
    "markdown_file_loader = DirectoryLoader(document_data_dir_path,\n",
    "                                       glob = file_globbers['markdown'],\n",
    "                                       show_progress = True,\n",
    "                                       loader_cls = UnstructuredMarkdownLoader)\n",
    "\n",
    "# Build a text file loader handling different text encodings\n",
    "text_loader_kwargs= {'autodetect_encoding': True}\n",
    "text_file_loader = DirectoryLoader(document_data_dir_path,\n",
    "                                   glob = file_globbers['text'],\n",
    "                                   show_progress = True,\n",
    "                                   loader_cls = TextLoader,\n",
    "                                   loader_kwargs=text_loader_kwargs)\n",
    "\n",
    "# Build a html file loader handling different text encodings\n",
    "html_file_loader = DirectoryLoader(document_data_dir_path,\n",
    "                                   glob = file_globbers['html'],\n",
    "                                   show_progress = True,\n",
    "                                   loader_cls = UnstructuredHTMLLoader)\n",
    "\n",
    "# Build a pdf file loader\n",
    "pdf_file_loader = DirectoryLoader(document_data_dir_path,\n",
    "                                  glob = file_globbers['pdf'],\n",
    "                                  show_progress = True,\n",
    "                                  loader_cls = PyMuPDFLoader)\n",
    "\n",
    "# Build docx file loader\n",
    "docx_file_loader = DirectoryLoader(document_data_dir_path,\n",
    "                                  glob = file_globbers['word_doc'],\n",
    "                                  show_progress = True,\n",
    "                                  loader_cls = Docx2txtLoader)\n",
    "\n",
    "# We also defined our text splitters here for each use-case\n",
    "default_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = n_size_in_doc_chunk,\n",
    "    chunk_overlap = n_size_in_chunk_overlap,\n",
    "    length_function = len,\n",
    "    is_separator_regex = False,\n",
    ")\n",
    "\n",
    "# Note we'd use rapidocr-onnxruntime too if we degrade python version to suit it as it needs <3.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:16<00:00, 16.29s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 71.42it/s]\n",
      "100%|██████████| 1/1 [00:12<00:00, 12.36s/it]\n",
      "100%|██████████| 3/3 [00:02<00:00,  1.46it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.67s/it]\n"
     ]
    }
   ],
   "source": [
    "loaded_markdown_files = markdown_file_loader.load()\n",
    "loaded_text_files = text_file_loader.load()\n",
    "loaded_html_files = html_file_loader.load()\n",
    "loaded_pdf_files = pdf_file_loader.load()\n",
    "loaded_docx_files = docx_file_loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the documents we go through text splitting, chunks and metadata extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text chunking is built together from langchain documentation here:\n",
    "# https://python.langchain.com/docs/modules/data_connection/document_transformers/\n",
    "# Note that it does propagate the metadata\n",
    "markdown_file_chunks = default_text_splitter.split_documents(loaded_markdown_files)\n",
    "text_file_chunks = default_text_splitter.split_documents(loaded_text_files)\n",
    "html_file_chunks = default_text_splitter.split_documents(loaded_html_files)\n",
    "pdf_file_chunks = default_text_splitter.split_documents(loaded_pdf_files)\n",
    "docx_file_chunks = default_text_splitter.split_documents(loaded_docx_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load up a local embedding model and then get ready to embed the text and save it to a chromadb.\n",
    "\n",
    "Some example interfaces considered from this [tutorial](https://realpython.com/chromadb-vector-database/#get-started-with-chromadb-an-open-source-vector-database).\n",
    "\n",
    "We also specifically referenced this [Google Collab example](https://colab.research.google.com/drive/17eByD88swEphf-1fvNOjf_C79k0h2DgF?usp=sharing#scrollTo=A-h1y_eAHmD-)\n",
    "which is linked to this [youtube video](https://www.youtube.com/watch?v=cFCGUjc33aU&t=242s) produced by Sam Witteveen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "persistent_chromadb_location = os.path.join(core_working_directory, 'doc_db')\n",
    "default_embedding_model = \"all-MiniLM-L6-v2\"\n",
    "document_collection_name = \"simple_test_of_chunks\"\n",
    "\n",
    "local_vector_db_client = chromadb.PersistentClient(path = persistent_chromadb_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old example aiming to use hugging face before realising instruct model needs modified package, not default sentence_transformers package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_instructor_document_task = \"Represent the general document chunk for retrieval:\"\n",
    "generic_instructor_query_task = \"Represent the general question for retrieving supporting documents:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "\n",
    "# Create a dictionary with model configuration options, specifying to use the CPU for computations\n",
    "embedding_model_kwargs = {'device': 'cpu'}\n",
    "\n",
    "# Create a dictionary with encoding options, specifically setting 'normalize_embeddings' to False\n",
    "embedding_encode_kwargs = {'normalize_embeddings': False,\n",
    "                           'batch_size': 1}\n",
    "\n",
    "# Initialize an instance of HuggingFaceEmbeddings with the specified parameters\n",
    "instructor_xl_embedding = HuggingFaceInstructEmbeddings(\n",
    "    model_name = embedding_model_path,\n",
    "    embed_instruction = generic_instructor_document_task,\n",
    "    query_instruction = generic_instructor_query_task,\n",
    "    model_kwargs = embedding_model_kwargs\n",
    ")\n",
    "# chromadb_instruct_xl_embedding = embedding_functions.InstructorEmbeddingFunction(\n",
    "#     model_name = embedding_model_path,\n",
    "#     device = \"cpu\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a document collection built off the local ChromaDB client and go through and embed all of our document chunks inside it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_chroma_db = Chroma(\n",
    "    client = local_vector_db_client,\n",
    "    collection_name = document_collection_name,\n",
    "    embedding_function = instructor_xl_embedding,\n",
    "    persist_directory = persistent_chromadb_location\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['9c78402e-a7bc-11ee-a061-4ccc6af94b0a',\n",
       " '9c78402f-a7bc-11ee-8c73-4ccc6af94b0a',\n",
       " '9c784030-a7bc-11ee-a476-4ccc6af94b0a',\n",
       " '9c784031-a7bc-11ee-b5a2-4ccc6af94b0a',\n",
       " '9c784032-a7bc-11ee-83c5-4ccc6af94b0a',\n",
       " '9c784033-a7bc-11ee-a1ff-4ccc6af94b0a',\n",
       " '9c784034-a7bc-11ee-a405-4ccc6af94b0a',\n",
       " '9c784035-a7bc-11ee-91ba-4ccc6af94b0a',\n",
       " '9c784036-a7bc-11ee-ab94-4ccc6af94b0a',\n",
       " '9c784037-a7bc-11ee-9be7-4ccc6af94b0a',\n",
       " '9c784038-a7bc-11ee-b5fc-4ccc6af94b0a',\n",
       " '9c784039-a7bc-11ee-a284-4ccc6af94b0a',\n",
       " '9c78403a-a7bc-11ee-abf3-4ccc6af94b0a',\n",
       " '9c78403b-a7bc-11ee-8676-4ccc6af94b0a',\n",
       " '9c78403c-a7bc-11ee-8644-4ccc6af94b0a',\n",
       " '9c78403d-a7bc-11ee-b503-4ccc6af94b0a',\n",
       " '9c78403e-a7bc-11ee-9f05-4ccc6af94b0a',\n",
       " '9c78403f-a7bc-11ee-8fb3-4ccc6af94b0a',\n",
       " '9c784040-a7bc-11ee-ae6d-4ccc6af94b0a',\n",
       " '9c784041-a7bc-11ee-a9f6-4ccc6af94b0a',\n",
       " '9c784042-a7bc-11ee-9254-4ccc6af94b0a',\n",
       " '9c784043-a7bc-11ee-bbbc-4ccc6af94b0a',\n",
       " '9c784044-a7bc-11ee-b9da-4ccc6af94b0a',\n",
       " '9c784045-a7bc-11ee-9c98-4ccc6af94b0a',\n",
       " '9c784046-a7bc-11ee-87e5-4ccc6af94b0a',\n",
       " '9c784047-a7bc-11ee-a938-4ccc6af94b0a',\n",
       " '9c784048-a7bc-11ee-afce-4ccc6af94b0a',\n",
       " '9c784049-a7bc-11ee-a52b-4ccc6af94b0a',\n",
       " '9c78404a-a7bc-11ee-a68a-4ccc6af94b0a',\n",
       " '9c78404b-a7bc-11ee-9bc9-4ccc6af94b0a',\n",
       " '9c78404c-a7bc-11ee-9ade-4ccc6af94b0a',\n",
       " '9c78404d-a7bc-11ee-a28a-4ccc6af94b0a',\n",
       " '9c78404e-a7bc-11ee-b905-4ccc6af94b0a',\n",
       " '9c78404f-a7bc-11ee-b7d4-4ccc6af94b0a',\n",
       " '9c784050-a7bc-11ee-bf97-4ccc6af94b0a',\n",
       " '9c784051-a7bc-11ee-a96d-4ccc6af94b0a',\n",
       " '9c784052-a7bc-11ee-b2bc-4ccc6af94b0a',\n",
       " '9c784053-a7bc-11ee-825a-4ccc6af94b0a',\n",
       " '9c784054-a7bc-11ee-ad78-4ccc6af94b0a',\n",
       " '9c784055-a7bc-11ee-919e-4ccc6af94b0a',\n",
       " '9c784056-a7bc-11ee-bb18-4ccc6af94b0a',\n",
       " '9c784057-a7bc-11ee-af8c-4ccc6af94b0a',\n",
       " '9c784058-a7bc-11ee-8e91-4ccc6af94b0a',\n",
       " '9c784059-a7bc-11ee-8cbe-4ccc6af94b0a',\n",
       " '9c78405a-a7bc-11ee-8bb9-4ccc6af94b0a',\n",
       " '9c78405b-a7bc-11ee-8203-4ccc6af94b0a',\n",
       " '9c78405c-a7bc-11ee-b9a0-4ccc6af94b0a',\n",
       " '9c78405d-a7bc-11ee-b067-4ccc6af94b0a',\n",
       " '9c78405e-a7bc-11ee-963e-4ccc6af94b0a',\n",
       " '9c78405f-a7bc-11ee-9a4c-4ccc6af94b0a',\n",
       " '9c784060-a7bc-11ee-8007-4ccc6af94b0a',\n",
       " '9c784061-a7bc-11ee-a800-4ccc6af94b0a',\n",
       " '9c784062-a7bc-11ee-998c-4ccc6af94b0a',\n",
       " '9c784063-a7bc-11ee-ae05-4ccc6af94b0a',\n",
       " '9c784064-a7bc-11ee-bace-4ccc6af94b0a',\n",
       " '9c784065-a7bc-11ee-ba93-4ccc6af94b0a',\n",
       " '9c784066-a7bc-11ee-8154-4ccc6af94b0a',\n",
       " '9c784067-a7bc-11ee-b243-4ccc6af94b0a',\n",
       " '9c784068-a7bc-11ee-b4c9-4ccc6af94b0a',\n",
       " '9c784069-a7bc-11ee-b61f-4ccc6af94b0a',\n",
       " '9c78406a-a7bc-11ee-adbb-4ccc6af94b0a',\n",
       " '9c78406b-a7bc-11ee-8343-4ccc6af94b0a',\n",
       " '9c78406c-a7bc-11ee-abe4-4ccc6af94b0a',\n",
       " '9c78406d-a7bc-11ee-aef8-4ccc6af94b0a',\n",
       " '9c78406e-a7bc-11ee-a93a-4ccc6af94b0a',\n",
       " '9c78406f-a7bc-11ee-9be8-4ccc6af94b0a',\n",
       " '9c784070-a7bc-11ee-86ef-4ccc6af94b0a',\n",
       " '9c784071-a7bc-11ee-9f2d-4ccc6af94b0a',\n",
       " '9c784072-a7bc-11ee-bdb1-4ccc6af94b0a',\n",
       " '9c784073-a7bc-11ee-abcb-4ccc6af94b0a',\n",
       " '9c784074-a7bc-11ee-8210-4ccc6af94b0a',\n",
       " '9c784075-a7bc-11ee-ba56-4ccc6af94b0a',\n",
       " '9c784076-a7bc-11ee-aa14-4ccc6af94b0a',\n",
       " '9c784077-a7bc-11ee-aca7-4ccc6af94b0a',\n",
       " '9c784078-a7bc-11ee-8d68-4ccc6af94b0a',\n",
       " '9c784079-a7bc-11ee-b6a8-4ccc6af94b0a',\n",
       " '9c78407a-a7bc-11ee-b5a0-4ccc6af94b0a',\n",
       " '9c78407b-a7bc-11ee-b11a-4ccc6af94b0a',\n",
       " '9c78407c-a7bc-11ee-98f2-4ccc6af94b0a',\n",
       " '9c78407d-a7bc-11ee-bdba-4ccc6af94b0a',\n",
       " '9c78407e-a7bc-11ee-9d3a-4ccc6af94b0a',\n",
       " '9c78407f-a7bc-11ee-9555-4ccc6af94b0a',\n",
       " '9c784080-a7bc-11ee-a41c-4ccc6af94b0a',\n",
       " '9c784081-a7bc-11ee-9903-4ccc6af94b0a',\n",
       " '9c784082-a7bc-11ee-a78c-4ccc6af94b0a',\n",
       " '9c784083-a7bc-11ee-986f-4ccc6af94b0a',\n",
       " '9c784084-a7bc-11ee-a619-4ccc6af94b0a',\n",
       " '9c784085-a7bc-11ee-999c-4ccc6af94b0a',\n",
       " '9c784086-a7bc-11ee-8411-4ccc6af94b0a',\n",
       " '9c784087-a7bc-11ee-b7cb-4ccc6af94b0a',\n",
       " '9c784088-a7bc-11ee-8ec8-4ccc6af94b0a',\n",
       " '9c784089-a7bc-11ee-9f14-4ccc6af94b0a',\n",
       " '9c78408a-a7bc-11ee-b46a-4ccc6af94b0a',\n",
       " '9c78408b-a7bc-11ee-a5a6-4ccc6af94b0a',\n",
       " '9c78408c-a7bc-11ee-8ec3-4ccc6af94b0a',\n",
       " '9c78408d-a7bc-11ee-bd44-4ccc6af94b0a',\n",
       " '9c78408e-a7bc-11ee-8aef-4ccc6af94b0a',\n",
       " '9c78408f-a7bc-11ee-9910-4ccc6af94b0a',\n",
       " '9c784090-a7bc-11ee-a568-4ccc6af94b0a',\n",
       " '9c784091-a7bc-11ee-b923-4ccc6af94b0a']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langchain_chroma_db.add_documents(\n",
    "    pdf_file_chunks[0:100]\n",
    ")\n",
    "\n",
    "# Note, langchain add documents approachs avoids trouble of manual adding such as via loop and\n",
    "# doc_collection_for_rag.add(\n",
    "#     ids = chunk_id,\n",
    "#     embeddings = chunk_context_embedding,a\n",
    "#     metadatas = chunk_metadata,\n",
    "#     documents = chunk_content_with_instruction\n",
    "# )\n",
    "# where I'd need to unpack those values from the langchain documents myself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4898"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checked count for runtime sense check to embed all three PDF documents\n",
    "len(pdf_file_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ask the lang-chain Chroma interface to perfect the file to disk after adding documents, clear it and then reload to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_chroma_db.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_chroma_db = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "495"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "# Clean up document datasets to save memory\n",
    "del loaded_markdown_files\n",
    "del loaded_text_files\n",
    "del loaded_html_files\n",
    "del loaded_pdf_files\n",
    "del loaded_docx_files\n",
    "del markdown_file_chunks\n",
    "del text_file_chunks\n",
    "del html_file_chunks\n",
    "del pdf_file_chunks\n",
    "del docx_file_chunks\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can load the persisted database from disk, and use it as normal. \n",
    "langchain_chroma_db = Chroma(\n",
    "    collection_name = document_collection_name,\n",
    "    persist_directory = persistent_chromadb_location, \n",
    "    embedding_function = instructor_xl_embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langchain_chroma_db._collection.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this persistent example restablished, we can swap the langchain Chroma interface into a retriever mode and both do a simple dummy example AND use it to build a simple Retrieval Q&A language chain with a local LLM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_retriever = langchain_chroma_db.as_retriever(search_kwargs = {\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "moby_dick_early_book_query = \"What project has made the book Mody Dick available?\"\n",
    "\n",
    "# Note that the hugging face interface silently handles adding the query instruction\n",
    "example_recovered_documents = document_retriever.get_relevant_documents(moby_dick_early_book_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='The Project Gutenberg eBook of Moby Dick; Or, The Whale \\n     \\nThis ebook is for the use of anyone anywhere in the United States and \\nmost other parts of the world at no cost and with almost no restrictions \\nwhatsoever. You may copy it, give it away or re-use it under the terms \\nof the Project Gutenberg License included with this ebook or online \\nat www.gutenberg.org. If you are not located in the United States, \\nyou will have to check the laws of the country where you are located \\nbefore using this eBook.', metadata={'author': 'Alexander Baker', 'creationDate': \"D:20231202201435+10'00'\", 'creator': 'Microsoft® Word for Office 365', 'file_path': 'C:\\\\Users\\\\Alex\\\\Google Drive\\\\projects\\\\llama2_retrieval_augmented_generation\\\\data\\\\documents\\\\moby_dick_3.pdf', 'format': 'PDF 1.7', 'keywords': '', 'modDate': \"D:20231202201435+10'00'\", 'page': 0, 'producer': 'Microsoft® Word for Office 365', 'source': 'C:\\\\Users\\\\Alex\\\\Google Drive\\\\projects\\\\llama2_retrieval_augmented_generation\\\\data\\\\documents\\\\moby_dick_3.pdf', 'subject': '', 'title': '', 'total_pages': 385, 'trapped': ''}),\n",
       " Document(page_content='The Project Gutenberg eBook of Moby Dick; Or, The Whale \\n     \\nThis ebook is for the use of anyone anywhere in the United States and \\nmost other parts of the world at no cost and with almost no restrictions \\nwhatsoever. You may copy it, give it away or re-use it under the terms \\nof the Project Gutenberg License included with this ebook or online \\nat www.gutenberg.org. If you are not located in the United States, \\nyou will have to check the laws of the country where you are located \\nbefore using this eBook.', metadata={'author': 'Alexander Baker', 'creationDate': \"D:20231202201435+10'00'\", 'creator': 'Microsoft® Word for Office 365', 'file_path': 'C:\\\\Users\\\\Alex\\\\Google Drive\\\\projects\\\\llama2_retrieval_augmented_generation\\\\data\\\\documents\\\\moby_dick_3.pdf', 'format': 'PDF 1.7', 'keywords': '', 'modDate': \"D:20231202201435+10'00'\", 'page': 0, 'producer': 'Microsoft® Word for Office 365', 'source': 'C:\\\\Users\\\\Alex\\\\Google Drive\\\\projects\\\\llama2_retrieval_augmented_generation\\\\data\\\\documents\\\\moby_dick_3.pdf', 'subject': '', 'title': '', 'total_pages': 385, 'trapped': ''}),\n",
       " Document(page_content='The Project Gutenberg eBook of Moby Dick; Or, The Whale \\n     \\nThis ebook is for the use of anyone anywhere in the United States and \\nmost other parts of the world at no cost and with almost no restrictions \\nwhatsoever. You may copy it, give it away or re-use it under the terms \\nof the Project Gutenberg License included with this ebook or online \\nat www.gutenberg.org. If you are not located in the United States, \\nyou will have to check the laws of the country where you are located \\nbefore using this eBook.', metadata={'author': 'Alexander Baker', 'creationDate': \"D:20231202201435+10'00'\", 'creator': 'Microsoft® Word for Office 365', 'file_path': 'C:\\\\Users\\\\Alex\\\\Google Drive\\\\projects\\\\llama2_retrieval_augmented_generation\\\\data\\\\documents\\\\moby_dick_3.pdf', 'format': 'PDF 1.7', 'keywords': '', 'modDate': \"D:20231202201435+10'00'\", 'page': 0, 'producer': 'Microsoft® Word for Office 365', 'source': 'C:\\\\Users\\\\Alex\\\\Google Drive\\\\projects\\\\llama2_retrieval_augmented_generation\\\\data\\\\documents\\\\moby_dick_3.pdf', 'subject': '', 'title': '', 'total_pages': 385, 'trapped': ''})]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_recovered_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deeper into building the RAG part of the example.\n",
    "\n",
    "The video series acting as a tutorial for this was (this youtube video)[https://www.youtube.com/watch?v=9ISVjh8mdlA] with this linked (Google Collab document)[https://colab.research.google.com/drive/1zG1R08TBikG05ecF8et4vi_1F9xutY-6?usp=sharing] by Sam Witteven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain import LLMChain\n",
    "from transformers import AutoTokenizer, pipeline, TextStreamer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Load in a local LLM and pair it with the chain \n",
    "llama2_model_tokeniser = AutoTokenizer.from_pretrained(llm_model_path)\n",
    "llama2_model_streamer = TextStreamer(llama2_model_tokeniser)\n",
    "\n",
    "llama2_local_model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path = llm_model_path,\n",
    "    # load_in_8bit = True,\n",
    "    # device_map = \"cpu\", # -1 should map to CPU \"auto\",\n",
    "    torch_dtype = torch.bfloat16,\n",
    "    low_cpu_mem_usage = False\n",
    ")\n",
    "\n",
    "llama2_model_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model = llama2_local_model,\n",
    "    tokenizer = llama2_model_tokeniser,\n",
    "    trust_remote_code = False,\n",
    "    max_length = 512,\n",
    "    do_sample = True,\n",
    "    top_k = 1,\n",
    "    num_return_sequences = 1,\n",
    "    eos_token_id = llama2_model_tokeniser.eos_token_id,\n",
    "    pad_token_id = llama2_model_tokeniser.eos_token_id,\n",
    "    streamer = llama2_model_streamer,\n",
    ")\n",
    "\n",
    "llama2_instruct_7b_llm = HuggingFacePipeline(pipeline = llama2_model_pipeline)\n",
    "\n",
    "llama2_local_model = None\n",
    "llama2_model_pipeline = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the chain to answer questions \n",
    "llama2_llm_instructor_embed_qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm = llama2_instruct_7b_llm, \n",
    "    chain_type = \"RAG\", \n",
    "    retriever = document_retriever, \n",
    "    return_source_documents = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cite sources\n",
    "\n",
    "import textwrap\n",
    "\n",
    "def wrap_text_preserve_newlines(text, width=110):\n",
    "    # Split the input text into lines based on newline characters\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    # Wrap each line individually\n",
    "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
    "\n",
    "    # Join the wrapped lines back together using newline characters\n",
    "    wrapped_text = '\\n'.join(wrapped_lines)\n",
    "\n",
    "    return wrapped_text\n",
    "\n",
    "def process_llm_response(llm_response):\n",
    "    print(wrap_text_preserve_newlines(llm_response['result']))\n",
    "    print('\\n\\nSources:')\n",
    "    for source in llm_response[\"source_documents\"]:\n",
    "        print(source.metadata['source'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-rag-demo-yaEi1c4i-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
